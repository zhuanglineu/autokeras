{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible for everyone. AutoKeras 1.0 is coming soon! Example Here is a short example of using the package. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) For detailed tutorial, please check here . Installation To install the package, please use the pip installation as follows: pip3 install autokeras # for 0.4 version pip3 install autokeras == 1 .0.0b0 # for 1.0 version Note: currently, AutoKeras is only compatible with: Python 3 . Cite this work Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} } Community You can use Gitter to communicate with people who are also interested in AutoKeras. You can also follow us on Twitter @autokeras for the latest news. Contributing Code You can follow the Contributing Guide for details. The easist way to contribute is to resolve the issues with the \" call for contributors \" tag. They are friendly to beginners. Support AutoKeras We accept donations on Open Collective . Thank every backer for supporting us! DISCLAIMER Please note that this is a pre-release version of the AutoKeras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \"as is\" and \"as available\" basis. AutoKeras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. AutoKeras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user's own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated. Acknowledgements The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Home"},{"location":"#autokeras-10-is-coming-soon","text":"","title":"AutoKeras 1.0 is coming soon!"},{"location":"#example","text":"Here is a short example of using the package. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) For detailed tutorial, please check here .","title":"Example"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip3 install autokeras # for 0.4 version pip3 install autokeras == 1 .0.0b0 # for 1.0 version Note: currently, AutoKeras is only compatible with: Python 3 .","title":"Installation"},{"location":"#cite-this-work","text":"Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} }","title":"Cite this work"},{"location":"#community","text":"You can use Gitter to communicate with people who are also interested in AutoKeras. You can also follow us on Twitter @autokeras for the latest news.","title":"Community"},{"location":"#contributing-code","text":"You can follow the Contributing Guide for details. The easist way to contribute is to resolve the issues with the \" call for contributors \" tag. They are friendly to beginners.","title":"Contributing Code"},{"location":"#support-autokeras","text":"We accept donations on Open Collective . Thank every backer for supporting us!","title":"Support AutoKeras"},{"location":"#disclaimer","text":"Please note that this is a pre-release version of the AutoKeras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \"as is\" and \"as available\" basis. AutoKeras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. AutoKeras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user's own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"DISCLAIMER"},{"location":"#acknowledgements","text":"The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Acknowledgements"},{"location":"about/","text":"About This package is developed by DATA LAB at Texas A&M University, collaborating with keras-team for version 1.0 and above. Core Team Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer. Top Contributors Tsung-Lin Yang Gabriel de Marmiesse Tao Wang Satya Kesav Boyuan Gong Yashwanth Reddy Cheng Cheng Yufei Wu Sirui Ding Praveen Kumar Contributor List","title":"About"},{"location":"about/#about","text":"This package is developed by DATA LAB at Texas A&M University, collaborating with keras-team for version 1.0 and above.","title":"About"},{"location":"about/#core-team","text":"Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"about/#top-contributors","text":"Tsung-Lin Yang Gabriel de Marmiesse Tao Wang Satya Kesav Boyuan Gong Yashwanth Reddy Cheng Cheng Yufei Wu Sirui Ding Praveen Kumar Contributor List","title":"Top Contributors"},{"location":"auto_model/","text":"[source] AutoModel class autokeras . AutoModel ( inputs , outputs , name = \"auto_model\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = \"greedy\" , overwrite = False , seed = None , ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Example # The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs : A list of Node instances. The input node(s) of the AutoModel. outputs : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner : String or subclass of AutoTuner. If use string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method AutoModel . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method AutoModel . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method AutoModel . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method AutoModel . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"AutoModel"},{"location":"auto_model/#automodel-class","text":"autokeras . AutoModel ( inputs , outputs , name = \"auto_model\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = \"greedy\" , overwrite = False , seed = None , ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Example # The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs : A list of Node instances. The input node(s) of the AutoModel. outputs : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner : String or subclass of AutoTuner. If use string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"AutoModel class"},{"location":"auto_model/#fit-method","text":"AutoModel . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"auto_model/#predict-method","text":"AutoModel . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"auto_model/#evaluate-method","text":"AutoModel . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"auto_model/#export_model-method","text":"AutoModel . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"base/","text":"[source] Node class autokeras . Node ( shape = None ) The nodes in a network connecting the blocks. [source] Preprocessor class autokeras . Preprocessor ( name = None , ** kwargs ) Hyper preprocessing block base class. It extends Block which extends Hypermodel. A preprocessor is a Hypermodel, which means it is a search space. However, different from other Hypermodels, it is also a model which can be fit. [source] Block class autokeras . Block ( name = None , ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. Arguments name : String. The name of the block. If unspecified, it will be set automatically with the class name. [source] build method Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source] Head class autokeras . Head ( loss = None , metrics = None , output_shape = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel. output_shape : Tuple of int(s). Defaults to None. If None, the output shape will be inferred from the AutoModel.","title":"Base Classes"},{"location":"base/#node-class","text":"autokeras . Node ( shape = None ) The nodes in a network connecting the blocks. [source]","title":"Node class"},{"location":"base/#preprocessor-class","text":"autokeras . Preprocessor ( name = None , ** kwargs ) Hyper preprocessing block base class. It extends Block which extends Hypermodel. A preprocessor is a Hypermodel, which means it is a search space. However, different from other Hypermodels, it is also a model which can be fit. [source]","title":"Preprocessor class"},{"location":"base/#block-class","text":"autokeras . Block ( name = None , ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. Arguments name : String. The name of the block. If unspecified, it will be set automatically with the class name. [source]","title":"Block class"},{"location":"base/#build-method","text":"Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source]","title":"build method"},{"location":"base/#head-class","text":"autokeras . Head ( loss = None , metrics = None , output_shape = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel. output_shape : Tuple of int(s). Defaults to None. If None, the output shape will be inferred from the AutoModel.","title":"Head class"},{"location":"block/","text":"[source] ConvBlock class autokeras . ConvBlock ( kernel_size = None , num_blocks = None , separable = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. [source] DenseBlock class autokeras . DenseBlock ( num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Block for Dense layers. Arguments num_layers : Int. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] EmbeddingBlock class autokeras . EmbeddingBlock ( max_features = None , pretraining = None , embedding_dim = None , dropout_rate = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. If not specified, we will use the vocabulary size in the preceding TextToIntSequence vocabulary size. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim : Int. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for after the Embedding layer. If left unspecified, it will be tuned automatically. [source] Merge class autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source] ResNetBlock class autokeras . ResNetBlock ( version = None , pooling = None , ** kwargs ) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source] RNNBlock class autokeras . RNNBlock ( return_sequences = False , bidirectional = None , num_layers = None , layer_type = None , ** kwargs ) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source] SpatialReduction class autokeras . SpatialReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] TemporalReduction class autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] XceptionBlock class autokeras . XceptionBlock ( activation = None , initial_strides = None , num_residual_blocks = None , pooling = None , ** kwargs ) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source] ImageBlock class autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , seed = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of HyperBlock to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source] StructuredDataBlock class autokeras . StructuredDataBlock ( feature_engineering = True , block_type = None , seed = None , ** kwargs ) Block for structured data. Arguments feature_engineering : Boolean. Whether to use feature engineering block. Defaults to True. If specified as None, it will be tuned automatically. block_type : String. 'dense' or 'lightgbm'. If it is 'dense', DenseBlock will be used. If it is 'lightgbm', LightGBM will be used. If unspecified, it will be tuned automatically. seed : Int. Random seed. [source] TextBlock class autokeras . TextBlock ( vectorizer = None , pretraining = None , ** kwargs ) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : Boolean. Whether to use pretraining weights in the N-gram vectorizer. If unspecified, it will be tuned automatically.","title":"Block"},{"location":"block/#convblock-class","text":"autokeras . ConvBlock ( kernel_size = None , num_blocks = None , separable = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. [source]","title":"ConvBlock class"},{"location":"block/#denseblock-class","text":"autokeras . DenseBlock ( num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Block for Dense layers. Arguments num_layers : Int. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"DenseBlock class"},{"location":"block/#embeddingblock-class","text":"autokeras . EmbeddingBlock ( max_features = None , pretraining = None , embedding_dim = None , dropout_rate = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. If not specified, we will use the vocabulary size in the preceding TextToIntSequence vocabulary size. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim : Int. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for after the Embedding layer. If left unspecified, it will be tuned automatically. [source]","title":"EmbeddingBlock class"},{"location":"block/#merge-class","text":"autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source]","title":"Merge class"},{"location":"block/#resnetblock-class","text":"autokeras . ResNetBlock ( version = None , pooling = None , ** kwargs ) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source]","title":"ResNetBlock class"},{"location":"block/#rnnblock-class","text":"autokeras . RNNBlock ( return_sequences = False , bidirectional = None , num_layers = None , layer_type = None , ** kwargs ) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source]","title":"RNNBlock class"},{"location":"block/#spatialreduction-class","text":"autokeras . SpatialReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"SpatialReduction class"},{"location":"block/#temporalreduction-class","text":"autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"TemporalReduction class"},{"location":"block/#xceptionblock-class","text":"autokeras . XceptionBlock ( activation = None , initial_strides = None , num_residual_blocks = None , pooling = None , ** kwargs ) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source]","title":"XceptionBlock class"},{"location":"block/#imageblock-class","text":"autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , seed = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of HyperBlock to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source]","title":"ImageBlock class"},{"location":"block/#structureddatablock-class","text":"autokeras . StructuredDataBlock ( feature_engineering = True , block_type = None , seed = None , ** kwargs ) Block for structured data. Arguments feature_engineering : Boolean. Whether to use feature engineering block. Defaults to True. If specified as None, it will be tuned automatically. block_type : String. 'dense' or 'lightgbm'. If it is 'dense', DenseBlock will be used. If it is 'lightgbm', LightGBM will be used. If unspecified, it will be tuned automatically. seed : Int. Random seed. [source]","title":"StructuredDataBlock class"},{"location":"block/#textblock-class","text":"autokeras . TextBlock ( vectorizer = None , pretraining = None , ** kwargs ) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : Boolean. Whether to use pretraining weights in the N-gram vectorizer. If unspecified, it will be tuned automatically.","title":"TextBlock class"},{"location":"contributing/","text":"Contributing Guide Contributions are welcome, and greatly appreciated! We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to AutoKeras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) to write the details of the feedback. Fix Bugs: You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whomever wants to fix. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , to ensure your pull request meet our requirements. Implement Features You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Unit tests are needed to test the new feature implemented and fully cover the new code you wrote. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , to ensure your pull request meet our requirements. Write Documentation The documentation of AutoKeras is either directly written into the Markdown files in templates directory , or automatically extracted from the docstrings by executing the autogen.py . Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide . Pull Request Guide Before you submit a pull request, check that it meets these guidelines: Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable and briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix. Code Style Guide Your code should pass the flake8 check. Docstrings should follow our style. Imports should follow our style. For \"our style\", just check the code of AutoKeras. Developer Tools Guide We highly recommend you to use Pycharm and virtualenvwrapper as well as a pre-commit hook. Visual Studio Code We suggest using Visual Studio Code (VSCode) to develop for AutoKeras. It can automatically format your code, sort your imports, mark any code style violations, generate docstrings according to our format for you. Check VSCode configuration here . virtualenv Use virtualenv to create a isolated Python environment for AutoKeras project to avoid dependency version conflicts with your other projects. Virtualenvwrapper is a even simpler command-line tool to manage multiple virtualenvs. pre-commit hook You can make git run flake8 before every commit automatically. It will make you go faster by avoiding pushing commit which are not passing the flake8 tests. To do this, open .git/hooks/pre-commit with a text editor and write flake8 inside. If the flake8 doesn't pass, the commit will be aborted. Main Contributor List We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Contributing Guide"},{"location":"contributing/#contributing-guide","text":"Contributions are welcome, and greatly appreciated! We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to AutoKeras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows.","title":"Contributing Guide"},{"location":"contributing/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) to write the details of the feedback.","title":"Submit Feedback"},{"location":"contributing/#fix-bugs","text":"You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whomever wants to fix. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , to ensure your pull request meet our requirements.","title":"Fix Bugs:"},{"location":"contributing/#implement-features","text":"You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Unit tests are needed to test the new feature implemented and fully cover the new code you wrote. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , to ensure your pull request meet our requirements.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"The documentation of AutoKeras is either directly written into the Markdown files in templates directory , or automatically extracted from the docstrings by executing the autogen.py . Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide .","title":"Write Documentation"},{"location":"contributing/#pull-request-guide","text":"Before you submit a pull request, check that it meets these guidelines: Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable and briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix.","title":"Pull Request Guide"},{"location":"contributing/#code-style-guide","text":"Your code should pass the flake8 check. Docstrings should follow our style. Imports should follow our style. For \"our style\", just check the code of AutoKeras.","title":"Code Style Guide"},{"location":"contributing/#developer-tools-guide","text":"We highly recommend you to use Pycharm and virtualenvwrapper as well as a pre-commit hook.","title":"Developer Tools Guide"},{"location":"contributing/#visual-studio-code","text":"We suggest using Visual Studio Code (VSCode) to develop for AutoKeras. It can automatically format your code, sort your imports, mark any code style violations, generate docstrings according to our format for you. Check VSCode configuration here .","title":"Visual Studio Code"},{"location":"contributing/#virtualenv","text":"Use virtualenv to create a isolated Python environment for AutoKeras project to avoid dependency version conflicts with your other projects. Virtualenvwrapper is a even simpler command-line tool to manage multiple virtualenvs.","title":"virtualenv"},{"location":"contributing/#pre-commit-hook","text":"You can make git run flake8 before every commit automatically. It will make you go faster by avoiding pushing commit which are not passing the flake8 tests. To do this, open .git/hooks/pre-commit with a text editor and write flake8 inside. If the flake8 doesn't pass, the commit will be aborted.","title":"pre-commit hook"},{"location":"contributing/#main-contributor-list","text":"We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Main Contributor List"},{"location":"docker/","text":"Auto-Keras Docker Download Auto-Keras Docker image The following command download Auto-Keras docker image to your machine. docker pull garawalid / autokeras : latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository Start Auto-Keras Docker container docker run - it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference ) Run application : To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run - it - v hostDir : / app --shm-size 2G garawalid/autokeras python file.py Example : Let's download the mnist example and run it within the container. Download the example : curl https : // raw . githubusercontent . com / keras - team / autokeras / master / examples / mnist . py --output mnist.py Run the mnist example : docker run - it - v \"$(pwd)\" : / app --shm-size 2G garawalid/autokeras python mnist.py","title":"Docker"},{"location":"docker/#auto-keras-docker","text":"","title":"Auto-Keras Docker"},{"location":"docker/#download-auto-keras-docker-image","text":"The following command download Auto-Keras docker image to your machine. docker pull garawalid / autokeras : latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository","title":"Download Auto-Keras Docker image"},{"location":"docker/#start-auto-keras-docker-container","text":"docker run - it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference )","title":"Start Auto-Keras Docker container"},{"location":"docker/#run-application","text":"To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run - it - v hostDir : / app --shm-size 2G garawalid/autokeras python file.py","title":"Run application :"},{"location":"docker/#example","text":"Let's download the mnist example and run it within the container. Download the example : curl https : // raw . githubusercontent . com / keras - team / autokeras / master / examples / mnist . py --output mnist.py Run the mnist example : docker run - it - v \"$(pwd)\" : / app --shm-size 2G garawalid/autokeras python mnist.py","title":"Example :"},{"location":"head/","text":"[source] ClassificationHead class autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout_rate = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] RegressionHead class autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout_rate = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use mean_squared_error . metrics : A list of Keras metrics. Defaults to use mean_squared_error . dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"Head"},{"location":"head/#classificationhead-class","text":"autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout_rate = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"ClassificationHead class"},{"location":"head/#regressionhead-class","text":"autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout_rate = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use mean_squared_error . metrics : A list of Keras metrics. Defaults to use mean_squared_error . dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"RegressionHead class"},{"location":"image_classifier/","text":"[source] ImageClassifier class autokeras . ImageClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , name = \"image_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method ImageClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method ImageClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method ImageClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageClassifier"},{"location":"image_classifier/#imageclassifier-class","text":"autokeras . ImageClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , name = \"image_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"ImageClassifier class"},{"location":"image_classifier/#fit-method","text":"ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"image_classifier/#predict-method","text":"ImageClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"image_classifier/#evaluate-method","text":"ImageClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"image_classifier/#export_model-method","text":"ImageClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"image_regressor/","text":"[source] ImageRegressor class autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method ImageRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method ImageRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method ImageRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageRegressor"},{"location":"image_regressor/#imageregressor-class","text":"autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"ImageRegressor class"},{"location":"image_regressor/#fit-method","text":"ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"image_regressor/#predict-method","text":"ImageRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"image_regressor/#evaluate-method","text":"ImageRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"image_regressor/#export_model-method","text":"ImageRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"install/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Installation"},{"location":"node/","text":"[source] ImageInput class autokeras . ImageInput ( shape = None ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. [source] Input class autokeras . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source] StructuredDataInput class autokeras . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source] TextInput class autokeras . TextInput ( shape = None ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.","title":"Node"},{"location":"node/#imageinput-class","text":"autokeras . ImageInput ( shape = None ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. [source]","title":"ImageInput class"},{"location":"node/#input-class","text":"autokeras . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source]","title":"Input class"},{"location":"node/#structureddatainput-class","text":"autokeras . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source]","title":"StructuredDataInput class"},{"location":"node/#textinput-class","text":"autokeras . TextInput ( shape = None ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.","title":"TextInput class"},{"location":"preprocessor/","text":"[source] FeatureEngineering class autokeras . FeatureEngineering ( max_columns = 1000 , ** kwargs ) A preprocessor block does feature engineering for the data. Arguments max_columns : Int. The maximum number of columns after feature engineering. Defaults to 1000. [source] ImageAugmentation class autokeras . ImageAugmentation ( percentage = 0.25 , rotation_range = 180 , random_crop = True , brightness_range = 0.5 , saturation_range = 0.5 , contrast_range = 0.5 , translation = True , horizontal_flip = True , vertical_flip = True , gaussian_noise = True , ** kwargs ) Collection of various image augmentation methods. Arguments percentage : Float. The percentage of data to augment. rotation_range : Int. The value can only be 0, 90, or 180. Degree range for random rotations. Default to 180. random_crop : Boolean. Whether to crop the image randomly. Default to True. brightness_range : Positive float. Serve as 'max_delta' in tf.image.random_brightness. Default to 0.5. Equivalent to adjust brightness using a 'delta' randomly picked in the interval [-max_delta, max_delta). saturation_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for saturation range. If given a tuple directly, it will serve as a range for picking a saturation shift value from. Default to 0.5. contrast_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for contrast range. If given a tuple directly, it will serve as a range for picking a contrast shift value from. Default to 0.5. translation : Boolean. Whether to translate the image. horizontal_flip : Boolean. Whether to flip the image horizontally. vertical_flip : Boolean. Whether to flip the image vertically. gaussian_noise : Boolean. Whether to add gaussian noise to the image. [source] LightGBM class autokeras . LightGBM ( seed = None , ** kwargs ) LightGBM Block for classification or regression task. Arguments seed : Int. Random seed. [source] Normalization class autokeras . Normalization ( ** kwargs ) Perform basic image transformation and augmentation. Arguments mean : Tensor. The mean value. Shape: (data last dimension length,) std : Tensor. The standard deviation. Shape is the same as mean. [source] TextToIntSequence class autokeras . TextToIntSequence ( max_len = None , ** kwargs ) Convert raw texts to sequences of word indices. [source] TextToNgramVector class autokeras . TextToNgramVector ( ngram_range = None , stop_words = None , max_features = 20000 , norm = \"l2\" , ** kwargs ) Convert raw texts to n-gram vectors. Arguments ngram_range : Int Tuple. Range of sizes of ngram tokens to be extracted. If not specified, it will be tuned automatically. Defaults to None. stop_words : Set or Iterable of strings. List of stop words to be removed during tokenization. Defaults to use regular expression \"(?u)\\b\\w\\w+\\b\". max_features : Positive Int. Maximum number of words to be considered during tokenization. Defaults to 20000. norm : String. Can be ('l1', 'l2' or None) Attribute to replicate normalization in sklearn TfidfVectorizer. Defaults to 'l2'.","title":"Preprocessor"},{"location":"preprocessor/#featureengineering-class","text":"autokeras . FeatureEngineering ( max_columns = 1000 , ** kwargs ) A preprocessor block does feature engineering for the data. Arguments max_columns : Int. The maximum number of columns after feature engineering. Defaults to 1000. [source]","title":"FeatureEngineering class"},{"location":"preprocessor/#imageaugmentation-class","text":"autokeras . ImageAugmentation ( percentage = 0.25 , rotation_range = 180 , random_crop = True , brightness_range = 0.5 , saturation_range = 0.5 , contrast_range = 0.5 , translation = True , horizontal_flip = True , vertical_flip = True , gaussian_noise = True , ** kwargs ) Collection of various image augmentation methods. Arguments percentage : Float. The percentage of data to augment. rotation_range : Int. The value can only be 0, 90, or 180. Degree range for random rotations. Default to 180. random_crop : Boolean. Whether to crop the image randomly. Default to True. brightness_range : Positive float. Serve as 'max_delta' in tf.image.random_brightness. Default to 0.5. Equivalent to adjust brightness using a 'delta' randomly picked in the interval [-max_delta, max_delta). saturation_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for saturation range. If given a tuple directly, it will serve as a range for picking a saturation shift value from. Default to 0.5. contrast_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for contrast range. If given a tuple directly, it will serve as a range for picking a contrast shift value from. Default to 0.5. translation : Boolean. Whether to translate the image. horizontal_flip : Boolean. Whether to flip the image horizontally. vertical_flip : Boolean. Whether to flip the image vertically. gaussian_noise : Boolean. Whether to add gaussian noise to the image. [source]","title":"ImageAugmentation class"},{"location":"preprocessor/#lightgbm-class","text":"autokeras . LightGBM ( seed = None , ** kwargs ) LightGBM Block for classification or regression task. Arguments seed : Int. Random seed. [source]","title":"LightGBM class"},{"location":"preprocessor/#normalization-class","text":"autokeras . Normalization ( ** kwargs ) Perform basic image transformation and augmentation. Arguments mean : Tensor. The mean value. Shape: (data last dimension length,) std : Tensor. The standard deviation. Shape is the same as mean. [source]","title":"Normalization class"},{"location":"preprocessor/#texttointsequence-class","text":"autokeras . TextToIntSequence ( max_len = None , ** kwargs ) Convert raw texts to sequences of word indices. [source]","title":"TextToIntSequence class"},{"location":"preprocessor/#texttongramvector-class","text":"autokeras . TextToNgramVector ( ngram_range = None , stop_words = None , max_features = 20000 , norm = \"l2\" , ** kwargs ) Convert raw texts to n-gram vectors. Arguments ngram_range : Int Tuple. Range of sizes of ngram tokens to be extracted. If not specified, it will be tuned automatically. Defaults to None. stop_words : Set or Iterable of strings. List of stop words to be removed during tokenization. Defaults to use regular expression \"(?u)\\b\\w\\w+\\b\". max_features : Positive Int. Maximum number of words to be considered during tokenization. Defaults to 20000. norm : String. Can be ('l1', 'l2' or None) Attribute to replicate normalization in sklearn TfidfVectorizer. Defaults to 'l2'.","title":"TextToNgramVector class"},{"location":"start/","text":"Getting Started Installation The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 . Latest Stable Version ( pip installation): You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras Bleeding Edge Version (manual installation): If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install - r requirements . txt python setup . py install A Simple Example We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs. Data with numpy array (.npy) format. [source] If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__' : ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) clf = ImageClassifier ( verbose = True ) clf . fit ( x_train , y_train , time_limit = 12 * 60 * 60 ) clf . final_fit ( x_train , y_train , x_test , y_test , retrain = True ) y = clf . evaluate ( x_test , y_test ) print ( y ) In the example above, the images and the labels are already formatted into numpy arrays. What if your data are raw image files ( e.g. .jpg, .png, .bmp)? [source] You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train , y_train = load_image_dataset ( csv_file_path = \"train/label.csv\" , images_path = \"train\" ) print ( x_train . shape ) print ( y_train . shape ) x_test , y_test = load_image_dataset ( csv_file_path = \"test/label.csv\" , images_path = \"test\" ) print ( x_test . shape ) print ( y_test . shape ) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name , Label 00000 . jpg , 5 00001 . jpg , 0 00002 . jpg , 4 00003 . jpg , 1 00004 . jpg , 9 00005 . jpg , 2 00006 . jpg , 1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500 class_1 \u2502 \u2502 class_1_image_1 . png \u2502 \u2502 class_1_image_2 . png | | ... \u2514\u2500\u2500\u2500 class_2 \u2502 class_2_image_1 . png \u2502 class_2_image_2 . png | ... The code below shows an example of how to create the CSV: train_dir = ' train ' # Path to the train directory class_dirs = [ i for i in os . listdir ( path = train_dir ) if os . path . isdir ( os . path . join ( train_dir , i )) ] with open ( ' train/label.csv ' , ' w ' ) as train_csv : fieldnames = [ ' File Name ' , ' Label ' ] writer = csv . DictWriter ( train_csv , fieldnames = fieldnames ) writer . writeheader () label = 0 for current_class in class_dirs : for image in os . listdir ( os . path . join ( train_dir , current_class )) : writer . writerow ( { ' File Name ' : str ( image ) , ' Label ' : label } ) label += 1 train_csv . close () Enable Multi-GPU Training Auto-Keras support multiple GPU training in the default setting. There's no additional step needed to enable multiple GPU training. However, if multiple-GPU training is not a desirable behavior. You can disable it via environmental variable. CUDA_VISIBLE_DEVICES . For example, in your bash: export CUDA_VISIBLE_DEVICES=0 . Keep in mind that when using multiple-GPU, make sure batch size is big enough that multiple-gpu context switch overhead won't effect the performance too much. Otherwise multiple-gpu training may be slower than single-GPU training. Portable Models How to export Portable model? [source] from autokeras import ImageClassifier clf = ImageClassifier ( verbose = True , augment = False ) clf . export_autokeras_model ( model_file_name ) The model will be stored into the path model_file_name . How to load exported Portable model? [source] from autokeras.utils import pickle_from_file model = pickle_from_file ( model_file_name ) results = model . evaluate ( x_test , y_test ) print ( results ) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised . Model Visualizations How to visualize the best selected architecture? [source] While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip : pip install graphviz conda : conda install - c conda - forge python - graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier ( path = \"~/automodels/\" , verbose = True , augment = False ) # Give a custom path of your choice clf . fit ( x_train , y_train , time_limit = 30 * 60 ) clf . final_fit ( x_train , y_train , x_test , y_test , retrain = True ) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == ' __main__ ' : visualize ( ' ~/automodels/ ' ) Net Modules MlpModule tutorial. [source] MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule ( loss , metric , searcher_args , path , verbose ) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule . fit ( n_output_node , input_shape , train_data , test_data , time_limit = 24 * 60 * 60 ) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule . final_fit ( train_data , test_data , trainer_args = None , retrain = False ) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model. CnnModule tutorial. [source] CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule ( loss = classification_loss , metric = Accuracy , searcher_args = {}, path = TEST_FOLDER , verbose = False ) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule . fit ( n_output_node , input_shape , train_data , test_data , time_limit = 24 * 60 * 60 ) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule . final_fit ( train_data , test_data , trainer_args = None , retrain = False ) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model. Task Modules Automated text classifier tutorial. [source] Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier ( verbose = True ) clf . fit ( x = x_train , y = y_train , time_limit = 12 * 60 * 60 ) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: * Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. * The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py . Pretrained Models Object detection tutorial. [source] by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&M. class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector () It will automatically download and load the weights into detector . Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Finally you can make predictions against an image: results = detector . predict ( \"/path/to/images/000001.jpg\" , output_file_path = \"/path/to/images/\" ) Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}] Sentiment Analysis tutorial. [source] The sentiment analysis module provides an interface to find the sentiment of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import SentimentAnalysis sentiment_analysis = SentimentAnalysis () During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls . predict ( \"The model is working well..\" ) Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment. Topic Classification tutorial. [source] The topic classifier module provides an interface to find the topic of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on AGNews dataset . Let\u2019s import the TopicClassifier module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import TopicClassifier topic_classifier = TopicClassifier () During initialization of TopicClassifier , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in TopicClassifier class on any input sentence provided as a string as shown below. The function returns one of the fours topics Business , Sci/Tech , World and Sports . class_name = topic_classifier . predict ( \"With some more practice, they will definitely make it to finals..\" ) If you run topic_classifier_example.py , you should see the predict function returns the label Sports , which is the predicted label for the input statement. Voice generator tutorial. [source] The voice generator is a refactor of deepvoice3 . The structure contains three main parts: Encoder : A fully-convolutional encoder, which converts textual features to an internallearned representation. Decoder : A fully-convolutional causal decoder, which decodes the learned representationwith a multi-hop convolutional attention mechanism into a low-dimensional audio repre-sentation (mel-scale spectrograms) in an autoregressive manner. Converter : A fully-convolutional post-processing network, which predicts final vocoderparameters (depending on the vocoder choice) from the decoder hidden states. Unlike thedecoder, the converter is non-causal and can thus depend on future context information For more details, please refer the original paper: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning Example: from autokeras.pretrained import VoiceGenerator voice_generator = VoiceGenerator () text = \"The approximation of pi is 3.14\" voice_generator . predict ( text , \"test.wav\" ) Voice recognizer tutorial. [source] The voice recognizer is a refactor of deepspeech . The model structure contains two parts: * Encoder: Convolutional layer followed by recurrent neural network and then fully convert network. Output is the hidden voice information. * Decoder: Decode the hidden voice information to the voice wave. For more details, please refer the original paper: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin Because currently torchaudio does not support pip install. So the current package doesn't support audio parsing part. To use the voice recognizer, one should first parse the audio following the standard below: First, install the torchaudio , the install process can refer the repo. Seconder use the following audio parser from autokeras.constant import Constant import torchaudio import scipy.signal import librosa import torch import numpy as np def load_audio ( path ): sound , _ = torchaudio . load ( path ) sound = sound . numpy () if len ( sound . shape ) > 1 : if sound . shape [ 0 ] == 1 : sound = sound . squeeze () else : sound = sound . mean ( axis = 0 ) # multiple channels, average return sound class SpectrogramParser : def __init__ ( self , audio_conf , normalize = False , augment = False ): \"\"\" Parses audio file into spectrogram with optional normalization and various augmentations :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds :param normalize(default False): Apply standard mean and deviation normalization to audio tensor :param augment(default False): Apply random tempo and gain perturbations \"\"\" super ( SpectrogramParser , self ) . __init__ () self . window_stride = audio_conf [ 'window_stride' ] self . window_size = audio_conf [ 'window_size' ] self . sample_rate = audio_conf [ 'sample_rate' ] self . window = scipy . signal . hamming self . normalize = normalize self . augment = augment self . noise_prob = audio_conf . get ( 'noise_prob' ) def parse_audio ( self , audio_path ): y = load_audio ( audio_path ) n_fft = int ( self . sample_rate * self . window_size ) win_length = n_fft hop_length = int ( self . sample_rate * self . window_stride ) # STFT D = librosa . stft ( y , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = self . window ) spect , _ = librosa . magphase ( D ) # S = log(S+1) spect = np . log1p ( spect ) spect = torch . FloatTensor ( spect ) if self . normalize : mean = spect . mean () std = spect . std () spect . add_ ( - mean ) spect . div_ ( std ) return spect parser = SpectrogramParser ( Constant . VOICE_RECONGINIZER_AUDIO_CONF , normalize = True ) spect = parser . parse_audio ( \"test.wav\" ) . contiguous () After this we will have the audio parsed as torch tensor in variable spect . Then we can use the following to recognize the voice: from autokeras.pretrained import VoiceRecognizer voice_recognizer = VoiceRecognizer () print ( voice_recognizer . predict ( audio_data = spect )) This voice recognizer pretrained model is well tuned based on the AN4 dataset. It has a large probability cannot perform well on other dataset.","title":"Getting Started 0.4"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 .","title":"Installation"},{"location":"start/#latest-stable-version-pip-installation","text":"You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras","title":"Latest Stable Version (pip installation):"},{"location":"start/#bleeding-edge-version-manual-installation","text":"If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install - r requirements . txt python setup . py install","title":"Bleeding Edge Version (manual installation):"},{"location":"start/#a-simple-example","text":"We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs.","title":"A Simple Example"},{"location":"start/#data-with-numpy-array-npy-format","text":"[source] If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__' : ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) clf = ImageClassifier ( verbose = True ) clf . fit ( x_train , y_train , time_limit = 12 * 60 * 60 ) clf . final_fit ( x_train , y_train , x_test , y_test , retrain = True ) y = clf . evaluate ( x_test , y_test ) print ( y ) In the example above, the images and the labels are already formatted into numpy arrays.","title":"Data with numpy array (.npy) format."},{"location":"start/#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp","text":"[source] You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train , y_train = load_image_dataset ( csv_file_path = \"train/label.csv\" , images_path = \"train\" ) print ( x_train . shape ) print ( y_train . shape ) x_test , y_test = load_image_dataset ( csv_file_path = \"test/label.csv\" , images_path = \"test\" ) print ( x_test . shape ) print ( y_test . shape ) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name , Label 00000 . jpg , 5 00001 . jpg , 0 00002 . jpg , 4 00003 . jpg , 1 00004 . jpg , 9 00005 . jpg , 2 00006 . jpg , 1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500 class_1 \u2502 \u2502 class_1_image_1 . png \u2502 \u2502 class_1_image_2 . png | | ... \u2514\u2500\u2500\u2500 class_2 \u2502 class_2_image_1 . png \u2502 class_2_image_2 . png | ... The code below shows an example of how to create the CSV: train_dir = ' train ' # Path to the train directory class_dirs = [ i for i in os . listdir ( path = train_dir ) if os . path . isdir ( os . path . join ( train_dir , i )) ] with open ( ' train/label.csv ' , ' w ' ) as train_csv : fieldnames = [ ' File Name ' , ' Label ' ] writer = csv . DictWriter ( train_csv , fieldnames = fieldnames ) writer . writeheader () label = 0 for current_class in class_dirs : for image in os . listdir ( os . path . join ( train_dir , current_class )) : writer . writerow ( { ' File Name ' : str ( image ) , ' Label ' : label } ) label += 1 train_csv . close ()","title":"What if your data are raw image files (e.g. .jpg, .png, .bmp)?"},{"location":"start/#enable-multi-gpu-training","text":"Auto-Keras support multiple GPU training in the default setting. There's no additional step needed to enable multiple GPU training. However, if multiple-GPU training is not a desirable behavior. You can disable it via environmental variable. CUDA_VISIBLE_DEVICES . For example, in your bash: export CUDA_VISIBLE_DEVICES=0 . Keep in mind that when using multiple-GPU, make sure batch size is big enough that multiple-gpu context switch overhead won't effect the performance too much. Otherwise multiple-gpu training may be slower than single-GPU training.","title":"Enable Multi-GPU Training"},{"location":"start/#portable-models","text":"","title":"Portable Models"},{"location":"start/#how-to-export-portable-model","text":"[source] from autokeras import ImageClassifier clf = ImageClassifier ( verbose = True , augment = False ) clf . export_autokeras_model ( model_file_name ) The model will be stored into the path model_file_name .","title":"How to export Portable model?"},{"location":"start/#how-to-load-exported-portable-model","text":"[source] from autokeras.utils import pickle_from_file model = pickle_from_file ( model_file_name ) results = model . evaluate ( x_test , y_test ) print ( results ) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised .","title":"How to load exported Portable model?"},{"location":"start/#model-visualizations","text":"","title":"Model Visualizations"},{"location":"start/#how-to-visualize-the-best-selected-architecture","text":"[source] While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip : pip install graphviz conda : conda install - c conda - forge python - graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier ( path = \"~/automodels/\" , verbose = True , augment = False ) # Give a custom path of your choice clf . fit ( x_train , y_train , time_limit = 30 * 60 ) clf . final_fit ( x_train , y_train , x_test , y_test , retrain = True ) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == ' __main__ ' : visualize ( ' ~/automodels/ ' )","title":"How to visualize the best selected architecture?"},{"location":"start/#net-modules","text":"","title":"Net Modules"},{"location":"start/#mlpmodule-tutorial","text":"[source] MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule ( loss , metric , searcher_args , path , verbose ) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule . fit ( n_output_node , input_shape , train_data , test_data , time_limit = 24 * 60 * 60 ) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule . final_fit ( train_data , test_data , trainer_args = None , retrain = False ) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model.","title":"MlpModule tutorial."},{"location":"start/#cnnmodule-tutorial","text":"[source] CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule ( loss = classification_loss , metric = Accuracy , searcher_args = {}, path = TEST_FOLDER , verbose = False ) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule . fit ( n_output_node , input_shape , train_data , test_data , time_limit = 24 * 60 * 60 ) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule . final_fit ( train_data , test_data , trainer_args = None , retrain = False ) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model.","title":"CnnModule tutorial."},{"location":"start/#task-modules","text":"","title":"Task Modules"},{"location":"start/#automated-text-classifier-tutorial","text":"[source] Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier ( verbose = True ) clf . fit ( x = x_train , y = y_train , time_limit = 12 * 60 * 60 ) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: * Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. * The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py .","title":"Automated text classifier tutorial."},{"location":"start/#pretrained-models","text":"","title":"Pretrained Models"},{"location":"start/#object-detection-tutorial","text":"[source]","title":"Object detection tutorial."},{"location":"start/#by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am","text":"class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector () It will automatically download and load the weights into detector . Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Finally you can make predictions against an image: results = detector . predict ( \"/path/to/images/000001.jpg\" , output_file_path = \"/path/to/images/\" ) Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}]","title":"by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M."},{"location":"start/#sentiment-analysis-tutorial","text":"[source] The sentiment analysis module provides an interface to find the sentiment of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import SentimentAnalysis sentiment_analysis = SentimentAnalysis () During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls . predict ( \"The model is working well..\" ) Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment.","title":"Sentiment Analysis tutorial."},{"location":"start/#topic-classification-tutorial","text":"[source] The topic classifier module provides an interface to find the topic of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on AGNews dataset . Let\u2019s import the TopicClassifier module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import TopicClassifier topic_classifier = TopicClassifier () During initialization of TopicClassifier , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in TopicClassifier class on any input sentence provided as a string as shown below. The function returns one of the fours topics Business , Sci/Tech , World and Sports . class_name = topic_classifier . predict ( \"With some more practice, they will definitely make it to finals..\" ) If you run topic_classifier_example.py , you should see the predict function returns the label Sports , which is the predicted label for the input statement.","title":"Topic Classification tutorial."},{"location":"start/#voice-generator-tutorial","text":"[source] The voice generator is a refactor of deepvoice3 . The structure contains three main parts: Encoder : A fully-convolutional encoder, which converts textual features to an internallearned representation. Decoder : A fully-convolutional causal decoder, which decodes the learned representationwith a multi-hop convolutional attention mechanism into a low-dimensional audio repre-sentation (mel-scale spectrograms) in an autoregressive manner. Converter : A fully-convolutional post-processing network, which predicts final vocoderparameters (depending on the vocoder choice) from the decoder hidden states. Unlike thedecoder, the converter is non-causal and can thus depend on future context information For more details, please refer the original paper: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning Example: from autokeras.pretrained import VoiceGenerator voice_generator = VoiceGenerator () text = \"The approximation of pi is 3.14\" voice_generator . predict ( text , \"test.wav\" )","title":"Voice generator tutorial."},{"location":"start/#voice-recognizer-tutorial","text":"[source] The voice recognizer is a refactor of deepspeech . The model structure contains two parts: * Encoder: Convolutional layer followed by recurrent neural network and then fully convert network. Output is the hidden voice information. * Decoder: Decode the hidden voice information to the voice wave. For more details, please refer the original paper: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin Because currently torchaudio does not support pip install. So the current package doesn't support audio parsing part. To use the voice recognizer, one should first parse the audio following the standard below: First, install the torchaudio , the install process can refer the repo. Seconder use the following audio parser from autokeras.constant import Constant import torchaudio import scipy.signal import librosa import torch import numpy as np def load_audio ( path ): sound , _ = torchaudio . load ( path ) sound = sound . numpy () if len ( sound . shape ) > 1 : if sound . shape [ 0 ] == 1 : sound = sound . squeeze () else : sound = sound . mean ( axis = 0 ) # multiple channels, average return sound class SpectrogramParser : def __init__ ( self , audio_conf , normalize = False , augment = False ): \"\"\" Parses audio file into spectrogram with optional normalization and various augmentations :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds :param normalize(default False): Apply standard mean and deviation normalization to audio tensor :param augment(default False): Apply random tempo and gain perturbations \"\"\" super ( SpectrogramParser , self ) . __init__ () self . window_stride = audio_conf [ 'window_stride' ] self . window_size = audio_conf [ 'window_size' ] self . sample_rate = audio_conf [ 'sample_rate' ] self . window = scipy . signal . hamming self . normalize = normalize self . augment = augment self . noise_prob = audio_conf . get ( 'noise_prob' ) def parse_audio ( self , audio_path ): y = load_audio ( audio_path ) n_fft = int ( self . sample_rate * self . window_size ) win_length = n_fft hop_length = int ( self . sample_rate * self . window_stride ) # STFT D = librosa . stft ( y , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = self . window ) spect , _ = librosa . magphase ( D ) # S = log(S+1) spect = np . log1p ( spect ) spect = torch . FloatTensor ( spect ) if self . normalize : mean = spect . mean () std = spect . std () spect . add_ ( - mean ) spect . div_ ( std ) return spect parser = SpectrogramParser ( Constant . VOICE_RECONGINIZER_AUDIO_CONF , normalize = True ) spect = parser . parse_audio ( \"test.wav\" ) . contiguous () After this we will have the audio parsed as torch tensor in variable spect . Then we can use the following to recognize the voice: from autokeras.pretrained import VoiceRecognizer voice_recognizer = VoiceRecognizer () print ( voice_recognizer . predict ( audio_data = spect )) This voice recognizer pretrained model is well tuned based on the AN4 dataset. It has a large probability cannot perform well on other dataset.","title":"Voice recognizer tutorial."},{"location":"structured_data_classifier/","text":"[source] StructuredDataClassifier class autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , overwrite = True , seed = None , ) AutoKeras structured data classification class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method StructuredDataClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method StructuredDataClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method StructuredDataClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataClassifier"},{"location":"structured_data_classifier/#structureddataclassifier-class","text":"autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , overwrite = True , seed = None , ) AutoKeras structured data classification class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"StructuredDataClassifier class"},{"location":"structured_data_classifier/#fit-method","text":"StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"structured_data_classifier/#predict-method","text":"StructuredDataClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"structured_data_classifier/#evaluate-method","text":"StructuredDataClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"structured_data_classifier/#export_model-method","text":"StructuredDataClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"structured_data_regressor/","text":"[source] StructuredDataRegressor class autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras structured data regression class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method StructuredDataRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method StructuredDataRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method StructuredDataRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataRegressor"},{"location":"structured_data_regressor/#structureddataregressor-class","text":"autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras structured data regression class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"StructuredDataRegressor class"},{"location":"structured_data_regressor/#fit-method","text":"StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"structured_data_regressor/#predict-method","text":"StructuredDataRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"structured_data_regressor/#evaluate-method","text":"StructuredDataRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"structured_data_regressor/#export_model-method","text":"StructuredDataRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"task/","text":"Task API AutoKeras support the following task APIs. {{autogenerated}} Coming Soon: StructuredDataClassifier StructuredDataRegressor TimeSeriesForecaster","title":"Task API"},{"location":"task/#task-api","text":"AutoKeras support the following task APIs. {{autogenerated}}","title":"Task API"},{"location":"task/#coming-soon","text":"StructuredDataClassifier StructuredDataRegressor TimeSeriesForecaster","title":"Coming Soon:"},{"location":"text_classifier/","text":"[source] TextClassifier class autokeras . TextClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , name = \"text_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method TextClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method TextClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method TextClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextClassifier"},{"location":"text_classifier/#textclassifier-class","text":"autokeras . TextClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , name = \"text_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"TextClassifier class"},{"location":"text_classifier/#fit-method","text":"TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"text_classifier/#predict-method","text":"TextClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"text_classifier/#evaluate-method","text":"TextClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"text_classifier/#export_model-method","text":"TextClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"text_regressor/","text":"[source] TextRegressor class autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method TextRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method TextRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method TextRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextRegressor"},{"location":"text_regressor/#textregressor-class","text":"autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"TextRegressor class"},{"location":"text_regressor/#fit-method","text":"TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"text_regressor/#predict-method","text":"TextRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"text_regressor/#evaluate-method","text":"TextRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"text_regressor/#export_model-method","text":"TextRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"examples/cifar10/","text":"import autokeras as ak def task_api (): ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () clf = ak . ImageClassifier ( seed = 5 , max_trials = 10 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) def io_api (): ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () clf = ak . AutoModel ( ak . ImageInput (), ak . ClassificationHead (), seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) def functional_api (): ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () input_node = ak . ImageInput () output_node = input_node output_node = ak . Normalization ()( output_node ) output_node = ak . ImageAugmentation ()( output_node ) output_node = ak . ResNetBlock ( version = 'next' )( output_node ) output_node = ak . SpatialReduction ()( output_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( input_node , output_node , seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) if __name__ == '__main__' : functional_api ()","title":"Cifar10"},{"location":"examples/imdb/","text":"import tensorflow as tf import autokeras as ak def imdb_raw (): max_features = 20000 index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = tf . keras . datasets . imdb . load_data ( num_words = max_features , index_from = index_offset ) x_train = x_train y_train = y_train . reshape ( - 1 , 1 ) x_test = x_test y_test = y_test . reshape ( - 1 , 1 ) word_to_id = tf . keras . datasets . imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) return ( x_train , y_train ), ( x_test , y_test ) def task_api (): ( x_train , y_train ), ( x_test , y_test ) = imdb_raw () clf = ak . TextClassifier ( max_trials = 3 , seed = 5 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) def functional_api (): max_features = 20000 max_words = 400 ( x_train , y_train ), ( x_test , y_test ) = tf . keras . datasets . imdb . load_data ( num_words = max_features , index_from = 3 ) x_train = tf . keras . preprocessing . sequence . pad_sequences ( x_train , maxlen = max_words ) x_test = tf . keras . preprocessing . sequence . pad_sequences ( x_test , maxlen = max_words ) print ( x_train . dtype ) print ( x_train [: 10 ]) input_node = ak . Input () output_node = input_node output_node = ak . EmbeddingBlock ( max_features = max_features )( output_node ) output_node = ak . ConvBlock ()( output_node ) output_node = ak . SpatialReduction ()( output_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( input_node , output_node , seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) if __name__ == '__main__' : task_api () # functional_api()","title":"Imdb"},{"location":"examples/mnist/","text":"import autokeras as ak def task_api (): ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () clf = ak . ImageClassifier ( seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) def io_api (): ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () clf = ak . AutoModel ( ak . ImageInput (), ak . ClassificationHead (), seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) def functional_api (): ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () input_node = ak . ImageInput () output_node = input_node output_node = ak . Normalization ()( output_node ) output_node = ak . ConvBlock ()( output_node ) output_node = ak . SpatialReduction ()( output_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( input_node , output_node , seed = 5 , max_trials = 3 ) clf . fit ( x_train , y_train , validation_split = 0.2 ) return clf . evaluate ( x_test , y_test ) if __name__ == '__main__' : task_api ()","title":"Mnist"},{"location":"examples/titanic/","text":"def from_csv (): clf = ak . StructuredDataClassifier ( seed = 5 , max_trials = 3 ) clf . fit ( x = 'tests/fixtures/titanic/train.csv' , y = 'survived' , validation_split = 0.2 ) clf . evaluate ( x = 'tests/fixtures/titanic/eval.csv' , y = 'survived' ) if __name__ == \"__main__\" : from_csv ()","title":"Titanic"},{"location":"tutorial/customized/","text":"In this tutorial, we show how to customize your search space with AutoModel and how to implement your own block as search space. This API is mainly for advanced users who already know what their model should look like. Customized Search Space First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train , y_train ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section . Validation Data If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation . Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Implement New Block You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. import autokeras as ak import tensorflow as tf class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . python . util . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( 'num_units' , min_value = 32 , max_value = 512 , step = 32 )) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , max_trials = 10 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train ) print ( auto_model . evaluate ( x_test , y_test )) Reference AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , EmbeddingBlock , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Customized Model"},{"location":"tutorial/customized/#customized-search-space","text":"First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train , y_train ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section .","title":"Customized Search Space"},{"location":"tutorial/customized/#validation-data","text":"If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation .","title":"Validation Data"},{"location":"tutorial/customized/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/customized/#implement-new-block","text":"You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. import autokeras as ak import tensorflow as tf class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . python . util . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( 'num_units' , min_value = 32 , max_value = 512 , step = 32 )) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , max_trials = 10 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train ) print ( auto_model . evaluate ( x_test , y_test ))","title":"Implement New Block"},{"location":"tutorial/customized/#reference","text":"AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , EmbeddingBlock , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Reference"},{"location":"tutorial/export/","text":"You can easily export your model the best model found by AutoKeras as a Keras Model. The following example uses ImageClassifier as an example. All the tasks and the AutoModel has this export_model function. from tensorflow.keras.datasets import mnist import autokeras as ak ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Export as a Keras Model. model = clf . export_model () print ( type ( model )) # <class 'tensorflow.python.keras.engine.training.Model'>","title":"Export Model"},{"location":"tutorial/image_classification/","text":"Image Classification A Simple Example The first step is to prepare your data. Here we use the MNIST dataset as an example. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier . import autokeras as ak # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier . You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = 'resnet' , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( percentage = 0.3 )( output_node ) output_node = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Data Format The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. import numpy as np eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . ImageClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Image Classification"},{"location":"tutorial/image_classification/#image-classification","text":"","title":"Image Classification"},{"location":"tutorial/image_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the MNIST dataset as an example. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier . import autokeras as ak # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/image_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/image_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier . You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = 'resnet' , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( percentage = 0.3 )( output_node ) output_node = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/image_classification/#data-format","text":"The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. import numpy as np eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . ImageClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/image_classification/#reference","text":"ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/image_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Image Regression"},{"location":"tutorial/multi/","text":"In this tutorial we are making use of the AutoModel API to show how to handle multi-modal data and multi-task. What is multi-modal? Multi-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data. What is multi-task? Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time. Data Preparation To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances ) Build and Train the Model Then we initialize the multi-modal and multi-task model with AutoModel . import autokeras as ak # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ 'mae' ]), ak . ClassificationHead ( loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) ], max_trials = 10 ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 10 ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(Feature Engineering) id8 --> id9(LightGBM) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) import autokeras as ak input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . FeatureEngineering ()( input_node2 ) output_node2 = ak . LightGBM ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [] output_node1 , output_node2 ], max_trials = 10 ) Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Reference AutoModel , ImageInput , StructuredDataInput , RegressionHead . ClassificationHead .","title":"Multi-Modal and Multi-Task"},{"location":"tutorial/multi/#what-is-multi-modal","text":"Multi-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data.","title":"What is multi-modal?"},{"location":"tutorial/multi/#what-is-multi-task","text":"Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time.","title":"What is multi-task?"},{"location":"tutorial/multi/#data-preparation","text":"To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances )","title":"Data Preparation"},{"location":"tutorial/multi/#build-and-train-the-model","text":"Then we initialize the multi-modal and multi-task model with AutoModel . import autokeras as ak # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ 'mae' ]), ak . ClassificationHead ( loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) ], max_trials = 10 ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 10 )","title":"Build and Train the Model"},{"location":"tutorial/multi/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/multi/#customized-search-space","text":"You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(Feature Engineering) id8 --> id9(LightGBM) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) import autokeras as ak input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . FeatureEngineering ()( input_node2 ) output_node2 = ak . LightGBM ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [] output_node1 , output_node2 ], max_trials = 10 )","title":"Customized Search Space"},{"location":"tutorial/multi/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/multi/#reference","text":"AutoModel , ImageInput , StructuredDataInput , RegressionHead . ClassificationHead .","title":"Reference"},{"location":"tutorial/overview/","text":"AutoKeras 1.0 Tutorial Supported Tasks AutoKeras supports several tasks with extremely simple interface. You can click the links below to see the detailed tutorial for each task. Suported Tasks : Image Classification , Image Regression , Text Classification , Text Regression , Structured Data Classification , Structured Data Regression . Coming Soon : Time Series Forcasting, Object Detection, Image Segmentation. Multi-Task and Multi-Modal Data If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details. Customized Model Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , EmbeddingBlock , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock . Heads : ClassificationHead , RegressionHead . Export Model You can follow this tutorial to export the best model.","title":"Overview"},{"location":"tutorial/overview/#autokeras-10-tutorial","text":"","title":"AutoKeras 1.0 Tutorial"},{"location":"tutorial/overview/#supported-tasks","text":"AutoKeras supports several tasks with extremely simple interface. You can click the links below to see the detailed tutorial for each task. Suported Tasks : Image Classification , Image Regression , Text Classification , Text Regression , Structured Data Classification , Structured Data Regression . Coming Soon : Time Series Forcasting, Object Detection, Image Segmentation.","title":"Supported Tasks"},{"location":"tutorial/overview/#multi-task-and-multi-modal-data","text":"If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details.","title":"Multi-Task and Multi-Modal Data"},{"location":"tutorial/overview/#customized-model","text":"Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , EmbeddingBlock , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock . Heads : ClassificationHead , RegressionHead .","title":"Customized Model"},{"location":"tutorial/overview/#export-model","text":"You can follow this tutorial to export the best model.","title":"Export Model"},{"location":"tutorial/structured_data_classification/","text":"Structured Data Classification A Simple Example The first step is to prepare your data. Here we use the Titanic dataset as an example. You can download the CSV files here . The second step is to run the StructuredDataClassifier . Replace all the /path/to with the path to the csv files. import autokeras as ak # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. '/path/to/train.csv' , # The name of the label column. 'survived' ) # Predict with the best model. predicted_y = clf . predict ( '/path/to/eval.csv' ) # Evaluate the best model with testing data. print ( clf . evaluate ( '/path/to/eval.csv' , 'survived' )) Data Format The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. import pandas as pd # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( 'train.csv' ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( 'survived' ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( 'eval.csv' ) y_test = x_test . pop ( 'survived' ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. Notably, the labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the Titanic dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ 'sex' , 'age' , 'n_siblings_spouses' , 'parch' , 'fare' , 'class' , 'deck' , 'embark_town' , 'alone' ], column_types = { 'sex' : 'categorical' , 'fare' : 'numerical' }, max_trials = 10 , # It tries 10 different models. ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., feature_engineering for whether to use the FeatureEngineering block, block_type for which type of block you want to use in the search space. You can use 'dense' for DenseBlock , or you can use 'lightgbm' for LightGBM . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( feature_engineering = False , block_type = 'dense' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . FeatureEngineering ( max_columns = 500 )( input_node ) output_node = ak . LightGBM ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Reference StructuredDataClassifier , AutoModel , StructuredDataClassifier , StructuredDataBlock , FeatureEngineering , DenseBlock , LightGBM , StructuredDataInput , ClassificationHead .","title":"Structured Data Classification"},{"location":"tutorial/structured_data_classification/#structured-data-classification","text":"","title":"Structured Data Classification"},{"location":"tutorial/structured_data_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the Titanic dataset as an example. You can download the CSV files here . The second step is to run the StructuredDataClassifier . Replace all the /path/to with the path to the csv files. import autokeras as ak # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. '/path/to/train.csv' , # The name of the label column. 'survived' ) # Predict with the best model. predicted_y = clf . predict ( '/path/to/eval.csv' ) # Evaluate the best model with testing data. print ( clf . evaluate ( '/path/to/eval.csv' , 'survived' ))","title":"A Simple Example"},{"location":"tutorial/structured_data_classification/#data-format","text":"The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. import pandas as pd # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( 'train.csv' ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( 'survived' ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( 'eval.csv' ) y_test = x_test . pop ( 'survived' ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. Notably, the labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the Titanic dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ 'sex' , 'age' , 'n_siblings_spouses' , 'parch' , 'fare' , 'class' , 'deck' , 'embark_town' , 'alone' ], column_types = { 'sex' : 'categorical' , 'fare' : 'numerical' }, max_trials = 10 , # It tries 10 different models. )","title":"Data Format"},{"location":"tutorial/structured_data_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/structured_data_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., feature_engineering for whether to use the FeatureEngineering block, block_type for which type of block you want to use in the search space. You can use 'dense' for DenseBlock , or you can use 'lightgbm' for LightGBM . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( feature_engineering = False , block_type = 'dense' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . FeatureEngineering ( max_columns = 500 )( input_node ) output_node = ak . LightGBM ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/structured_data_classification/#reference","text":"StructuredDataClassifier , AutoModel , StructuredDataClassifier , StructuredDataBlock , FeatureEngineering , DenseBlock , LightGBM , StructuredDataInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/structured_data_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Structured Data Regression"},{"location":"tutorial/text_classification/","text":"Text Classification A Simple Example The first step is to prepare your data. Here we use the IMDB dataset as an example. import numpy as np from tensorflow.keras.datasets import imdb # Load the integer sequence the IMDB dataset with Keras. index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = 1000 , index_from = index_offset ) y_train = y_train . reshape ( - 1 , 1 ) y_test = y_test . reshape ( - 1 , 1 ) # Prepare the dictionary of index to word. word_to_id = imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} # Convert the word indices to words. x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextClassifier . import autokeras as ak # Initialize the text classifier. clf = ak . TextClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the text classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use EmbeddingBlock for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextBlock ( vectorizer = 'ngram' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . EmbeddingBlock ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Data Format The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the IMDB dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . TextClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference TextClassifier , AutoModel , TextBlock , TextToInteSequence , EmbeddingBlock , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Text Classification"},{"location":"tutorial/text_classification/#text-classification","text":"","title":"Text Classification"},{"location":"tutorial/text_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the IMDB dataset as an example. import numpy as np from tensorflow.keras.datasets import imdb # Load the integer sequence the IMDB dataset with Keras. index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = 1000 , index_from = index_offset ) y_train = y_train . reshape ( - 1 , 1 ) y_test = y_test . reshape ( - 1 , 1 ) # Prepare the dictionary of index to word. word_to_id = imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} # Convert the word indices to words. x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextClassifier . import autokeras as ak # Initialize the text classifier. clf = ak . TextClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the text classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/text_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/text_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use EmbeddingBlock for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextBlock ( vectorizer = 'ngram' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . EmbeddingBlock ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/text_classification/#data-format","text":"The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the IMDB dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . TextClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/text_classification/#reference","text":"TextClassifier , AutoModel , TextBlock , TextToInteSequence , EmbeddingBlock , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/text_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Text Regression"}]}